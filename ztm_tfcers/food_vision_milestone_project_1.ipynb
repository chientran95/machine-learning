{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLaDq25mykWN"
   },
   "source": [
    "## Check GPU\n",
    "Mixed precision training need GPu with compute capability of 7.0+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656057250570,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "VAC_5rYJicZ4",
    "outputId": "0b198301-420d-4961-f0c1-dd31c2609218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-25824f3f-d8e8-604b-a4ea-62f14d22994d)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 74491,
     "status": "ok",
     "timestamp": 1656054759264,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "4-vT5xZee2Mr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "36200c2e-df4a-4d21-e035-6b80e869700d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow==2.4.1\n",
      "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.3 MB 13 kB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.37.1)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.8.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 63.9 MB/s \n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 51.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.17.3)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 68.0 MB/s \n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 51.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 45.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.7)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.23.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.6.15)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.0)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68720 sha256=de56f1558f69511ddd6339c97ba8cdc96b1f90a08f49954b9100fb8321cb52f5\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built wrapt\n",
      "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.46.3\n",
      "    Uninstalling grpcio-1.46.3:\n",
      "      Successfully uninstalled grpcio-1.46.3\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.1.0\n",
      "    Uninstalling absl-py-1.1.0:\n",
      "      Successfully uninstalled absl-py-1.1.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.8.0\n",
      "    Uninstalling tensorflow-estimator-2.8.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.3\n",
      "    Uninstalling gast-0.5.3:\n",
      "      Successfully uninstalled gast-0.5.3\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
      "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
      "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 tensorflow-2.4.1 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy",
         "typing_extensions"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Mix precision training have error with tensorflow 2.5+, so we use tensorflow 2.4.1\n",
    "!pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7475,
     "status": "ok",
     "timestamp": 1656057261975,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "8LpEDWLxKg46",
    "outputId": "75bb1371-0193-455f-bb1f-592d58505bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1480,
     "status": "ok",
     "timestamp": 1656057267202,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "ZqKKuFt7zYvf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chientm/miniconda3/envs/sandbox/lib/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5BE7WYl9b_8"
   },
   "source": [
    "## Use TensorFlow Datasets to Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1656057269443,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "YDMExkAG8ztE"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1656057269444,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "gXA8b2619s0X",
    "outputId": "132cb33e-61f1-4637-bccf-2ee4d9ffd165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "datasets_list = tfds.list_builders() # get all available datasets in TFDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3804,
     "status": "ok",
     "timestamp": 1656057276049,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "ClXZDWng-s8F"
   },
   "outputs": [],
   "source": [
    "(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n",
    "                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n",
    "                                             shuffle_files=True, # shuffle files on download?\n",
    "                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n",
    "                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1656057276049,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "Zoy8Tu7VR2ji",
    "outputId": "7b234262-0d66-4657-dbb1-ac27a143d991"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=101),\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656057276049,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "g2UkCaLsDXaR",
    "outputId": "19147f47-6558-417a-82ae-d94a3e1ed481"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_pie',\n",
       " 'baby_back_ribs',\n",
       " 'baklava',\n",
       " 'beef_carpaccio',\n",
       " 'beef_tartare',\n",
       " 'beet_salad',\n",
       " 'beignets',\n",
       " 'bibimbap',\n",
       " 'bread_pudding',\n",
       " 'breakfast_burrito']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ds_info.features[\"label\"].names\n",
    "class_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeRJnQMIYLcy"
   },
   "source": [
    "## Create preprocessing functions for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1656057289832,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "NKuwdjm0CWc1"
   },
   "outputs": [],
   "source": [
    "def preprocess_img(image, label, img_shape=224):\n",
    "  \"\"\"\n",
    "  Converts image datatype from 'uint8' -> 'float32' and reshapes image to\n",
    "  [img_shape, img_shape, color_channels]\n",
    "  \"\"\"\n",
    "  image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n",
    "  return tf.cast(image, tf.float32), label # return (float32_image, label) tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2rd4_3CjdGE"
   },
   "source": [
    "## Batch & prepare datasets\n",
    "\n",
    "We're going to be using:\n",
    "\n",
    "* [`map()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) - maps a predefined function to a target dataset (e.g. `preprocess_img()` to our image tensors)\n",
    "* [`shuffle()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) - randomly shuffles the elements of a target dataset up `buffer_size` (ideally, the `buffer_size` is equal to the size of the dataset, however, this may have implications on memory)\n",
    "* [`batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) - turns elements of a target dataset into batches (size defined by parameter `batch_size`)\n",
    "* [`prefetch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) - prepares subsequent batches of data whilst other batches of data are being computed on (improves data loading speed but costs memory)\n",
    "* Extra: [`cache()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) - caches elements in a target dataset, saving loading time (will only if your dataset is small enough to fit in memory, standard Colab instances only have 12GB of memory) \n",
    "\n",
    "Things to note:\n",
    "- `shuffle()` keeps a buffer of the number you pass it images shuffled, ideally this number would be all of the samples in your training set, however, if your training set is large, this buffer might not fit in memory (a fairly large number like 1000 or 10000 is usually suffice for shuffling)\n",
    "- For methods with the `num_parallel_calls` parameter available (such as `map()`), setting it to`num_parallel_calls=tf.data.AUTOTUNE` will parallelize preprocessing and significantly improve speed\n",
    "- Can't use `cache()` unless your dataset can fit in memory\n",
    "\n",
    "Woah, the above is alot. But once we've coded below, it'll start to make sense.\n",
    "\n",
    "We're going to through things in the following order:\n",
    "\n",
    "```\n",
    "Original dataset (e.g. train_data) -> map() -> shuffle() -> batch() -> prefetch() -> PrefetchDataset\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1656057302990,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "VhA4gq-pI2W3"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj3umnpMvSw8"
   },
   "source": [
    "## Create modelling callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1656057327833,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "wyYmxPnlXOwd"
   },
   "outputs": [],
   "source": [
    "# TensorBoard callback\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# ModelCheckpoint callback\n",
    "checkpoint_path = \"model_checkpoints/feat_extractor/cp.ckpt\" # saving weights requires \".ckpt\" extension\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                      montior=\"val_acc\", # save the model weights with best validation accuracy\n",
    "                                                      save_best_only=True, # only save the best weights\n",
    "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
    "                                                      verbose=0) # don't print out whether or not model is being saved "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyXlCU50UElG"
   },
   "source": [
    "## Setup mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1656057334126,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "5BuEjmlybR7V",
    "outputId": "bb8f2d7d-76e9-4a07-ca8f-51154624347d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(policy=\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1656057335410,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "qzSWJP8KkKae",
    "outputId": "c30294f1-db79-485e-e6c6-2d5e09e47fec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Policy \"mixed_float16\">"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_precision.global_policy() # should output \"mixed_float16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA8FBJwwvVoG"
   },
   "source": [
    "## Build feature extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2709,
     "status": "ok",
     "timestamp": 1656057340859,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "GrkWpCzfXKE7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
    "# x = preprocessing.Rescaling(1./255)(x)\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
    "x = layers.Dense(len(class_names))(x)\n",
    "# Separate activation of output layer so we can output float32 activations\n",
    "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656057340859,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "wfEG8ud_jsNY",
    "outputId": "35d3f2b8-3317-40cc-8b5e-01dfc75487cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb0 (Functional)  (None, None, None, 1280)  4049571   \n",
      "_________________________________________________________________\n",
      "pooling_layer (GlobalAverage (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 101)               129381    \n",
      "_________________________________________________________________\n",
      "softmax_float32 (Activation) (None, 101)               0         \n",
      "=================================================================\n",
      "Total params: 4,178,952\n",
      "Trainable params: 129,381\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1656057340859,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "Zk__ebBLHC-Q",
    "outputId": "6e31520e-77e0-4852-e608-3d539ba862ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer True float32 <Policy \"float32\">\n",
      "efficientnetb0 False float32 <Policy \"mixed_float16\">\n",
      "pooling_layer True float32 <Policy \"mixed_float16\">\n",
      "dense True float32 <Policy \"mixed_float16\">\n",
      "softmax_float32 True float32 <Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJz5S66ojyUS"
   },
   "source": [
    "## Fit the feature extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 592027,
     "status": "ok",
     "timestamp": 1656057953790,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "4v7rXZG-ZkNJ",
    "outputId": "8d8db0fb-4872-4f4f-c2fc-d79d263003d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard_logs/feat_extract/20220624-075601\n",
      "Epoch 1/3\n",
      "2368/2368 [==============================] - 211s 77ms/step - loss: 2.3259 - accuracy: 0.4647 - val_loss: 1.2352 - val_accuracy: 0.6758\n",
      "Epoch 2/3\n",
      "2368/2368 [==============================] - 175s 73ms/step - loss: 1.3110 - accuracy: 0.6621 - val_loss: 1.1277 - val_accuracy: 0.6992\n",
      "Epoch 3/3\n",
      "2368/2368 [==============================] - 203s 85ms/step - loss: 1.1501 - accuracy: 0.6992 - val_loss: 1.0809 - val_accuracy: 0.7129\n"
     ]
    }
   ],
   "source": [
    "# Fit the feature extraction model for 3 epochs with tensorboard and model checkpoint callbacks\n",
    "history_feat_extractor = model.fit(train_data, epochs=3, validation_data=test_data, validation_steps=int(0.15*len(test_data)),\n",
    "                                   callbacks=[create_tensorboard_callback(dir_name='tensorboard_logs', experiment_name='feat_extract'), model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80199,
     "status": "ok",
     "timestamp": 1656058033985,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "jhV7fvTreV27",
    "outputId": "64a4b92d-0928-45e6-ab36-7569e42e3cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/790 [==============================] - 80s 101ms/step - loss: 1.0881 - accuracy: 0.7078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0880811214447021, 0.7077623605728149]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model (unsaved version) on whole test dataset\n",
    "feat_extract_res = model.evaluate(test_data)\n",
    "feat_extract_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvTGiIFv3eOe"
   },
   "source": [
    "## Save the whole model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50855,
     "status": "ok",
     "timestamp": 1656056028200,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "RHKn4Ex57wzF",
    "outputId": "2dbeead1-332c-4029-9cbf-ac8d79c592dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: feat_extractor_trained/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: feat_extractor_trained/assets\n"
     ]
    }
   ],
   "source": [
    "# Save model locally (if you're using Google Colab, your saved model will Colab instance terminates)\n",
    "model.save('feat_extractor_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKGDBKrU6rej"
   },
   "outputs": [],
   "source": [
    "# Load model previously saved above\n",
    "loaded_model = tf.keras.models.load_model('feat_extractor_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1656056175965,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "fUJXpxmMKnYV",
    "outputId": "7ee43807-e99a-42af-aed7-b270d95db8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer True float32 <Policy \"float32\">\n",
      "efficientnetb0 True float32 <Policy \"mixed_float16\">\n",
      "pooling_layer True float32 <Policy \"mixed_float16\">\n",
      "dense True float32 <Policy \"mixed_float16\">\n",
      "softmax_float32 True float32 <Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "# Check the layers in the base model and see what dtype policy they're using\n",
    "# Check the dtype_policy attributes of layers in our model\n",
    "for layer in loaded_model.layers:\n",
    "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68064,
     "status": "ok",
     "timestamp": 1656056327362,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "5Qym9gSm6vL_",
    "outputId": "d8d742a4-7341-4fd4-c023-b3deb3318042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/790 [==============================] - 68s 83ms/step - loss: 1.0926 - accuracy: 0.7041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0925607681274414, 0.7040792107582092]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loaded model performance (this should be the same as results_feature_extract_model)\n",
    "loaded_feat_extract_res = loaded_model.evaluate(test_data)\n",
    "loaded_feat_extract_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1656056328480,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "4BUhHSXI-l0v",
    "outputId": "3f777fbc-5762-4cde-ed56-f9096b797f4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isclose(feat_extract_res, loaded_feat_extract_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj21OVxBGlw9"
   },
   "source": [
    "## Preparing our model's layers for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1656058033986,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "S-RQ633CapPk"
   },
   "outputs": [],
   "source": [
    "# Set all of the layers .trainable variable in the loaded model to True (so they're unfrozen)\n",
    "model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1656058033986,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "xsm5BtWCM5RX",
    "outputId": "8dd74556-c5d0-429f-9dfe-b6126ab42580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer True float32 <Policy \"float32\">\n",
      "efficientnetb0 True float32 <Policy \"mixed_float16\">\n",
      "pooling_layer True float32 <Policy \"mixed_float16\">\n",
      "dense True float32 <Policy \"mixed_float16\">\n",
      "softmax_float32 True float32 <Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1656058033986,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "GcKFVlXVwjJy"
   },
   "outputs": [],
   "source": [
    "# EarlyStopping callback\n",
    "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
    "\n",
    "\n",
    "# ModelCheckpoint callback\n",
    "checkpoint_path = \"model_checkpoints/finetuned_model/cp.ckpt\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, montior=\"val_loss\", mode='min',\n",
    "                                                      save_best_only=True, save_weights_only=True, verbose=0)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=2,\n",
    "                                                 min_lr=1e-7, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1656058033986,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "GlpO9LflcVHW"
   },
   "outputs": [],
   "source": [
    "# Use the Adam optimizer with a 10x lower than default learning rate\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522845,
     "status": "ok",
     "timestamp": 1656059568092,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "LkUtOdVkbMPC",
    "outputId": "eb3c26f1-51dd-457c-92bf-4f240dfd8f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard_logs/finetune_model/20220624-080725\n",
      "Epoch 3/100\n",
      "2368/2368 [==============================] - 356s 142ms/step - loss: 0.9829 - accuracy: 0.7368 - val_loss: 0.8057 - val_accuracy: 0.7728\n",
      "Epoch 4/100\n",
      "2368/2368 [==============================] - 291s 122ms/step - loss: 0.5776 - accuracy: 0.8411 - val_loss: 0.7817 - val_accuracy: 0.7873\n",
      "Epoch 5/100\n",
      "2368/2368 [==============================] - 291s 122ms/step - loss: 0.3261 - accuracy: 0.9080 - val_loss: 0.8727 - val_accuracy: 0.7773\n",
      "Epoch 6/100\n",
      "2368/2368 [==============================] - 291s 122ms/step - loss: 0.1648 - accuracy: 0.9520 - val_loss: 0.9651 - val_accuracy: 0.7773\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 7/100\n",
      "2368/2368 [==============================] - 290s 122ms/step - loss: 0.0508 - accuracy: 0.9885 - val_loss: 1.0222 - val_accuracy: 0.8085\n"
     ]
    }
   ],
   "source": [
    "# Start to fine-tune (all layers)\n",
    "# Use 100 epochs as the default, Validate on 15% of the test_data\n",
    "finetune_history = model.fit(train_data, epochs=100, initial_epoch=history_feat_extractor.epoch[-1],\n",
    "                             validation_data=test_data, validation_steps=int(0.15*len(test_data)),\n",
    "                             callbacks=[create_tensorboard_callback(dir_name='tensorboard_logs', experiment_name='finetune_model'),\n",
    "                                        model_checkpoint, early_stopping_callbacks, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33288,
     "status": "ok",
     "timestamp": 1656059602400,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "K1As0OhYHFX-",
    "outputId": "a58c744c-0a36-47b0-b97e-49b22c7c3ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: finetune_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: finetune_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('finetune_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50548,
     "status": "ok",
     "timestamp": 1656059652938,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "2CR6q8MYM37K",
    "outputId": "f8e45f22-29ad-4718-a998-3c29d86f027b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/790 [==============================] - 50s 64ms/step - loss: 1.0164 - accuracy: 0.8015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0164353847503662, 0.8015049695968628]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_res = model.evaluate(test_data)\n",
    "finetune_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFjooc5Gy0I2"
   },
   "source": [
    "## View training results on TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35502,
     "status": "ok",
     "timestamp": 1656060293256,
     "user": {
      "displayName": "Minh Chien Tran",
      "userId": "00764184138496707751"
     },
     "user_tz": -420
    },
    "id": "cCpVz0GSPucB",
    "outputId": "c2e4e4e1-51f9-427e-a0df-e5f9cdfdfccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-24 08:44:20.659551: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\n",
      "***** TensorBoard Uploader *****\n",
      "\n",
      "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
      "the following directory:\n",
      "\n",
      "./tensorboard_logs\n",
      "\n",
      "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
      "data.\n",
      "\n",
      "Your use of this service is subject to Google's Terms of Service\n",
      "<https://policies.google.com/terms> and Privacy Policy\n",
      "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
      "<https://tensorboard.dev/policy/terms/>.\n",
      "\n",
      "This notice will not be shown again while you are logged into the uploader.\n",
      "To log out, run `tensorboard dev auth revoke`.\n",
      "\n",
      "Continue? (yes/NO) yes\n",
      "\n",
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=RAoz9PIy82qe2kgC4XLoNn7tZzuTGm&prompt=consent&access_type=offline\n",
      "Enter the authorization code: 4/1AX4XfWgBAp8QiAq0JBCw56WgOnGntkGkj0bZTmmhd0wpej8IKod3qUrgUwQ\n",
      "\n",
      "\n",
      "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/e3wxbumWRmKN9we14UGx7w/\n",
      "\n",
      "\u001b[1m[2022-06-24T08:44:49]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2022-06-24T08:44:52]\u001b[0m Total uploaded: 44 scalars, 0 tensors, 3 binary objects (2.8 MB)\n",
      "\u001b[1m[2022-06-24T08:44:52]\u001b[0m Done scanning logdir.\n",
      "\n",
      "\n",
      "Done. View your TensorBoard at https://tensorboard.dev/experiment/e3wxbumWRmKN9we14UGx7w/\n"
     ]
    }
   ],
   "source": [
    "!tensorboard dev upload --logdir ./tensorboard_logs \\\n",
    "  --name \"Fine-tuning EfficientNetB0 on all Food101 Data\" \\\n",
    "  --description \"Training results for fine-tuning EfficientNetB0 on Food101 Data with learning rate 0.0001\" \\\n",
    "  --one_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE17Yr3POZSH"
   },
   "source": [
    "## More evaluate trained model\n",
    "\n",
    "Some ideas you might want to go through:\n",
    "1. Find the precision, recall and f1 scores for each class (all 101).\n",
    "2. Build a confusion matrix for each of the classes.\n",
    "3. Find your model's *most wrong* predictions (those with the highest prediction probability but the wrong prediction).\n",
    "\n",
    "See the evaluation section at the end of [Transfer Learning Part 3: Scaling Up for more](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84H_oXV8PqSy"
   },
   "outputs": [],
   "source": [
    "# YOUR_CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TEMPLATE_07_food_vision_milestone_project_1.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
