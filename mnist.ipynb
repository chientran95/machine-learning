{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport math, time, cv2, itertools\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pdb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "['train.csv', 'sample_submission.csv', 'test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "812dc3f35fec4236a6c3f75ef1229afd0e307198"
      },
      "cell_type": "code",
      "source": "train_data = pd.read_csv('../input/train.csv')\n# test_data = pd.read_csv('../input/test.csv')",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "185558a8df3748e709565fb8017818f3bb7c510c"
      },
      "cell_type": "code",
      "source": "y = pd.get_dummies(train_data.label).values\nX = train_data.iloc[:, 1:].values\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, random_state = 0)\nprint(type(train_X), type(train_y), type(val_X), type(val_y))\nprint(train_X.shape, train_y.shape, val_X.shape, val_y.shape)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n  FutureWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n(33600, 784) (33600, 10) (8400, 784) (8400, 10)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "61c4da78d97ff93e1bbf7949e5b4c82e38ba3355"
      },
      "cell_type": "code",
      "source": "def next_batch(step, batch_size, data, labels):\n    return data[step*batch_size:(step + 1)*batch_size], labels[step*batch_size:(step + 1)*batch_size]",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e4b08f5c953b8e4ec5a08568b482dd86eb67fa06"
      },
      "cell_type": "markdown",
      "source": " ## Simple linear model"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "x_input = tf.placeholder(tf.float32, shape=[None, 784])\ny_label = tf.placeholder(tf.float32, shape=[None, 10])\n\n#W = tf.Variable(tf.zeros([784, 10]))\n#b = tf.Variable(tf.zeros([10]))\n\nW = tf.Variable(tf.truncated_normal([784, 10], stddev=0.1))\nb = tf.Variable(tf.constant(0.1, shape=[10]))\n\ny_pred = tf.matmul(x_input, W) + b\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69db635cbd4bc06f8733212d5b29eef03290435d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_label, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "399062626fbb63a2ca67d2cd9891a796c67ddc15",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "epochs = 10\nbatch_size = 100\nsteps = math.ceil(train_X.shape[0] // batch_size)\n\ntrain_accs = []\neval_accs = []\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        for step in range(steps):\n            batch_X, batch_y = next_batch(step, batch_size, train_X, train_y)\n            #if step % 50 == 0:\n            #    train_accuracy = accuracy.eval(feed_dict={x_input: batch_X, y_label: batch_y})\n            #    print('step %d, training accuracy %g' % (step, train_accuracy))\n            train_step.run(feed_dict={x_input: batch_X, y_label: batch_y})\n        train_accuracy = accuracy.eval(feed_dict={x_input: batch_X, y_label: batch_y})\n        eval_accuracy = accuracy.eval(feed_dict={x_input: val_X, y_label: val_y})\n        train_accs.append(train_accuracy)\n        eval_accs.append(eval_accuracy)\n        print('Epoch: %d - Accuracy: %g' % (epoch, eval_accuracy))\n\nx = np.array(range(epochs))\nplt.plot(x, train_accs)\nplt.plot(x, eval_accs)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d4b3d17c93e1beca5d905047db5f6fd2c94c4db5"
      },
      "cell_type": "markdown",
      "source": "## Convolutional Network"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "35251d4487057fc574f43ad4cbec6e2bafc1fe01"
      },
      "cell_type": "code",
      "source": "def weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b31186ed2ad84490d786d28f2b802abd8ff95fe",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Input\nx_input = tf.placeholder(tf.float32, shape=[None, 784])\ny_label = tf.placeholder(tf.float32, shape=[None, 10])\nx_image = tf.reshape(x_input, [-1, 28, 28, 1])\n\n# Convoution layer 1\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# Convoution layer 2\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# FC layer\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# Dropout\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# Softmax layer\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "efd0d80faf1be3acd6ef85bfdd9ca378c6c169ce",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_label, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nepochs = 10\nbatch_size = 100\nsteps = math.ceil(train_X.shape[0] // batch_size)\n\ntrain_accs = []\neval_accs = []\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        for step in range(steps):\n            batch_X, batch_y = next_batch(step, batch_size, train_X, train_y)\n            #if step % 50 == 0:\n            #    train_accuracy = accuracy.eval(feed_dict={x_input: batch_X, y_label: batch_y, keep_prob: 1.0})\n            #    print('step %d, training accuracy %g' % (step, train_accuracy))\n            train_step.run(feed_dict={x_input: batch_X, y_label: batch_y, keep_prob: 0.5})\n        train_accuracy = accuracy.eval(feed_dict={x_input: batch_X, y_label: batch_y, keep_prob: 1.0})\n        eval_accuracy = accuracy.eval(feed_dict={x_input: val_X, y_label: val_y, keep_prob: 1.0})\n        train_accs.append(train_accuracy)\n        eval_accs.append(eval_accuracy)\n        print('Epoch: %d - Accuracy: %g' % (epoch, eval_accuracy))\n        \nx = np.array(range(epochs))\nplt.plot(x, train_accs)\nplt.plot(x, eval_accs)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "5fb04687a1221f387e1da3f85051697cf733ebb6"
      },
      "cell_type": "markdown",
      "source": "## DCGAN"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "00f1868a4236d1028d903637150aa7bc8730e304"
      },
      "cell_type": "code",
      "source": "def lrelu(x, th=0.2):\n    return tf.maximum(th * x, x)\n\ndef generator(x, isTrain=True, reuse=False):\n    with tf.variable_scope('generator', reuse=reuse):\n\n        # 1st hidden layer\n        conv1 = tf.layers.conv2d_transpose(x, 1024, [4, 4], strides=(1, 1), padding='valid')\n        lrelu1 = lrelu(tf.layers.batch_normalization(conv1, training=isTrain), 0.2)\n\n        # 2nd hidden layer\n        conv2 = tf.layers.conv2d_transpose(lrelu1, 512, [4, 4], strides=(2, 2), padding='same')\n        lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training=isTrain), 0.2)\n\n        # 3rd hidden layer\n        conv3 = tf.layers.conv2d_transpose(lrelu2, 256, [4, 4], strides=(2, 2), padding='same')\n        lrelu3 = lrelu(tf.layers.batch_normalization(conv3, training=isTrain), 0.2)\n\n        # 4th hidden layer\n        conv4 = tf.layers.conv2d_transpose(lrelu3, 128, [4, 4], strides=(2, 2), padding='same')\n        lrelu4 = lrelu(tf.layers.batch_normalization(conv4, training=isTrain), 0.2)\n\n        # output layer\n        conv5 = tf.layers.conv2d_transpose(lrelu4, 1, [4, 4], strides=(2, 2), padding='same')\n        o = tf.nn.tanh(conv5)\n\n        return o\n    \ndef discriminator(x, isTrain=True, reuse=False):\n    with tf.variable_scope('discriminator', reuse=reuse):\n        # 1st hidden layer\n        conv1 = tf.layers.conv2d(x, 128, [4, 4], strides=(2, 2), padding='same')\n        lrelu1 = lrelu(conv1, 0.2)\n\n        # 2nd hidden layer\n        conv2 = tf.layers.conv2d(lrelu1, 256, [4, 4], strides=(2, 2), padding='same')\n        lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training=isTrain), 0.2)\n\n        # 3rd hidden layer\n        conv3 = tf.layers.conv2d(lrelu2, 512, [4, 4], strides=(2, 2), padding='same')\n        lrelu3 = lrelu(tf.layers.batch_normalization(conv3, training=isTrain), 0.2)\n\n        # 4th hidden layer\n        conv4 = tf.layers.conv2d(lrelu3, 1024, [4, 4], strides=(2, 2), padding='same')\n        lrelu4 = lrelu(tf.layers.batch_normalization(conv4, training=isTrain), 0.2)\n\n        # output layer\n        conv5 = tf.layers.conv2d(lrelu4, 1, [4, 4], strides=(1, 1), padding='valid')\n        o = tf.nn.sigmoid(conv5)\n\n        return o, conv5",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "40b2087d6d839b3d5ab8b4e329d2a695d47aabac"
      },
      "cell_type": "code",
      "source": "fixed_z_ = np.random.normal(0, 1, (25, 1, 1, 100))\ndef show_result(num_epoch, show = False, save = False, path = 'result.png'):\n    test_images = sess.run(G_z, {z: fixed_z_, isTrain: False})\n    if num_epoch == 20:\n        pdb.set_trace()\n\n    size_figure_grid = 5\n    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\n\n    for k in range(size_figure_grid*size_figure_grid):\n        i = k // size_figure_grid\n        j = k % size_figure_grid\n        ax[i, j].cla()\n        ax[i, j].imshow(np.reshape(test_images[k] * 255, (64, 64)), cmap='gray')\n\n    label = 'Epoch' + str(num_epoch)\n    fig.text(0.5, 0.04, label, ha='center')\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n        \ndef show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n    x = range(len(hist['D_losses']))\n\n    y1 = hist['D_losses']\n    y2 = hist['G_losses']\n\n    plt.plot(x, y1, label='D_loss')\n    plt.plot(x, y2, label='G_loss')\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save:\n        plt.savefig(path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6999d2cfa8eb15672aefe30936ffdeccb80bee1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# training parameters\nbatch_size = 100\nlr = 0.0002\ntrain_epoch = 20\nsteps = math.ceil(train_X.shape[0] // batch_size)\n\n# variables : input\nx = tf.placeholder(tf.float32, shape=(None, 64, 64, 1))\nz = tf.placeholder(tf.float32, shape=(None, 1, 1, 100))\nisTrain = tf.placeholder(dtype=tf.bool)\n\n# networks : generator\nG_z = generator(z, isTrain)\n\n# networks : discriminator\nD_real, D_real_logits = discriminator(x, isTrain)\nD_fake, D_fake_logits = discriminator(G_z, isTrain, reuse=True)\n\n# loss for each network\nD_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits\n                             (logits=D_real_logits, labels=tf.ones([batch_size, 1, 1, 1]))\n                            )\nD_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits\n                             (logits=D_fake_logits, labels=tf.zeros([batch_size, 1, 1, 1]))\n                            )\nD_loss = D_loss_real + D_loss_fake\nG_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits\n                        (logits=D_fake_logits, labels=tf.ones([batch_size, 1, 1, 1]))\n                       )\n\n# trainable variables for each network\nT_vars = tf.trainable_variables()\nD_vars = [var for var in T_vars if var.name.startswith('discriminator')]\nG_vars = [var for var in T_vars if var.name.startswith('generator')]\n\n# optimizer for each network\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n    D_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(D_loss, var_list=D_vars)\n    G_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(G_loss, var_list=G_vars)\n\n# open session and initialize all variables\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3399cdf4e29efeb5b203b15726aeb9b3b3986981",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# MNIST resize and normalization\nx_input = tf.placeholder(tf.float32, shape=[None, 784])\nx_reshaped = tf.reshape(x_input, [-1, 28, 28, 1])\ntrain_set = tf.image.resize_images(x_reshaped, [64, 64]).eval(feed_dict={x_input: train_X})\ntrain_set = (train_set - 0.5) / 0.5\n\ntrain_hist = {}\ntrain_hist['D_losses'] = []\ntrain_hist['G_losses'] = []\ntrain_hist['per_epoch_ptimes'] = []\ntrain_hist['total_ptime'] = []",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d85a663a6e4338a3f7ffd37d1013b765fcda5d61"
      },
      "cell_type": "code",
      "source": "root = 'MNIST_DCGAN_results/'\nmodel = 'MNIST_DCGAN_'\nif not os.path.isdir(root):\n    os.mkdir(root)\nif not os.path.isdir(root + 'Fixed_results'):\n    os.mkdir(root + 'Fixed_results')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8194c89c070b48ab2dfdcbe3532d0f5e22ee7e0d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# training-loop\nnp.random.seed(int(time.time()))\nprint('training start!')\nstart_time = time.time()\nfor epoch in range(train_epoch):\n    G_losses = []\n    D_losses = []\n    epoch_start_time = time.time()\n    for step in range(steps):\n        # update discriminator\n        batch_X, batch_y = next_batch(step, batch_size, train_set, train_y)\n        z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n\n        loss_d_, _ = sess.run([D_loss, D_optim], {x: batch_X, z: z_, isTrain: True})\n        D_losses.append(loss_d_)\n\n        # update generator\n        z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n        loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: batch_X, isTrain: True})\n        G_losses.append(loss_g_)\n\n    epoch_end_time = time.time()\n    per_epoch_ptime = epoch_end_time - epoch_start_time\n    print('[%d/%d] - training time: %.2f loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses)))\n    #fixed_p = root + 'Fixed_results/' + model + str(epoch + 1) + '.png'\n    show_result((epoch + 1), save=False, show=True)\n    train_hist['D_losses'].append(np.mean(D_losses))\n    train_hist['G_losses'].append(np.mean(G_losses))\n    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n\nend_time = time.time()\ntotal_ptime = end_time - start_time\ntrain_hist['total_ptime'].append(total_ptime)\n\nprint('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))\n#print(\"Training finish!... save training results\")\n#with open(root + model + 'train_hist.pkl', 'wb') as f:\n#    pickle.dump(train_hist, f)\n\nshow_train_hist(train_hist, save=False, show=True)\n\n#images = []\n#for e in range(train_epoch):\n#    img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n#    images.append(imageio.imread(img_name))\n#imageio.mimsave(root + model + 'generation_animation.gif', images, fps=5)\n\nsess.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b5c6f8ceb8f7441764a5cc9c2808ba288a8e73aa"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}