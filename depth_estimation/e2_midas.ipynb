{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21ddcf-fee9-4ff9-92e2-ae311a5b166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from metrics import Metric\n",
    "from utils import process_depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be4569-ce91-42e8-83f7-c11489a22d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
    "model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
    "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
    "\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f950f-7eda-4281-bf6d-3857a8a775bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = '00019_00183_indoors_000_010.png'\n",
    "filename2 = 'dog.jpg'\n",
    "img1 = cv2.imread(filename1)\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.imread(filename2)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "input_batch1 = transform(img1).to(device)\n",
    "input_batch2 = transform(img2).to(device)\n",
    "input_batch1.shape, img1.shape, input_batch2.shape, img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480f207-0fd1-461e-8aa5-f2b2df1cf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_df.csv')\n",
    "data = df[260:].reset_index(drop=\"true\")\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31be5e4-96f2-464b-8d69-a51fa7fe9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_normalize(data, origin_range, target_range):\n",
    "    ori_min, ori_max = origin_range\n",
    "    tar_min, tar_max = target_range\n",
    "    return tar_min + (data - ori_min) * ((tar_max - tar_min) / (ori_max - ori_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3315d8-4b34-4443-bb39-fe00312d8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = {'img_path': [], 'mean_absrel': [], 'mean_sqrel': [], 'rms': [], 'rms_log': [], 'delta1': [], 'delta2': [], 'delta3': []}\n",
    "with torch.no_grad():\n",
    "    for idx, row in data.iterrows():\n",
    "        image_ = cv2.imread(row['image'])\n",
    "        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "        input_image = transform(image_).to(device)\n",
    "        h, w = input_image.shape[-2:]\n",
    "\n",
    "        depth_map = process_depth_map(row['depth'], row['mask'], (w,h))\n",
    "        \n",
    "        pred = midas(input_image)\n",
    "        pred_np = pred.permute(1, 2, 0).numpy()\n",
    "        norm_pred = general_normalize(pred_np, (pred_np.min(), pred_np.max()), (depth_map.min(), depth_map.max()))\n",
    "        \n",
    "        metric = Metric(depth_map, norm_pred)\n",
    "        evaluation_metrics['img_path'].append(row['image'])\n",
    "        evaluation_metrics['mean_absrel'].append(metric.mean_absrel_err())\n",
    "        evaluation_metrics['mean_sqrel'].append(metric.mean_sqrel_err())\n",
    "        evaluation_metrics['rms'].append(metric.rms_err())\n",
    "        evaluation_metrics['rms_log'].append(metric.rms_log_err())\n",
    "        evaluation_metrics['delta1'].append(metric.percentage_of_pixel_acc()['delta1'])\n",
    "        evaluation_metrics['delta2'].append(metric.percentage_of_pixel_acc()['delta2'])\n",
    "        evaluation_metrics['delta3'].append(metric.percentage_of_pixel_acc()['delta3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930e793-8695-46f1-b0a7-3df3d27a6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(evaluation_metrics)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7436a9c-9b43-4c73-8883-6664972b3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('metric_midas.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079addb-420c-4fd2-a6b3-6b1f210a7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, depth_gts, depth_preds = [], [], []\n",
    "with torch.no_grad():\n",
    "    for idx, row in data.iterrows():\n",
    "        if idx >= 4:\n",
    "            break\n",
    "        image_ = cv2.imread(row['image'])\n",
    "        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "        input_image = transform(image_).to(device)\n",
    "        h, w = input_image.shape[-2:]\n",
    "\n",
    "        depth_map = process_depth_map(row['depth'], row['mask'], (w,h))\n",
    "        \n",
    "        pred = midas(input_image)\n",
    "        pred_np = pred.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        imgs.append(image_)\n",
    "        depth_gts.append(depth_map)\n",
    "        depth_preds.append(pred_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702343d5-5ab3-4b48-90ac-b46859349259",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(18, 18))\n",
    "\n",
    "i = 0\n",
    "ax[0].imshow(imgs[i])\n",
    "ax[1].imshow(depth_gts[i])\n",
    "ax[2].imshow(depth_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba935ff-4f94-407b-97e4-19ce3bd600bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(18, 18))\n",
    "\n",
    "i = 0\n",
    "ax[0].imshow(imgs[i])\n",
    "ax[1].imshow(depth_gts[i])\n",
    "ax[2].imshow(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47853f-607c-4046-9c17-16727f641dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
